{"doc_id": "doc_000001", "title": "ColBERT v2", "text": "ColBERTv2 is a retrieval model that balances the expressiveness of computationally expensive models with the efficiency of less expressive but faster models. It introduces a \"late interaction\" mechanism over BERT-based embeddings.\n\nStandard dense retrieval models, often called bi-encoders, independently embed the query and the document into single-vector representations. During search, they compute a single similarity score (like dot-product or cosine similarity) between the query vector and all document vectors. This is fast but can lose nuance, as the embedding must compress the entire meaning of the text into one vector.\n\nOn the other hand, cross-encoders (or re-rankers) jointly process the query and a document. This allows for deeper, token-level comparisons, resulting in much higher accuracy. However, this process is extremely slow and not feasible for searching over millions of documents.\n\nColBERT offers a middle ground. It produces multi-vector embeddings for both the query and the document. For a query, each of its tokens is converted into an embedding. For a document, ColBERT also embeds each of its tokens. The \"late interaction\" happens at search time. For each query embedding, ColBERT finds the maximum similarity score against all document embeddings. These maximum scores are then summed up to get the final query-document score.\n\nThis approach is more granular than bi-encoders because it allows for token-level matching, but it's much faster than cross-encoders because the expensive joint processing is replaced by a simple MaxSim operation. ColBERTv2 further optimizes this by pre-computing document embeddings and using a compressed representation for the index, making it both fast and memory-efficient."}
{"doc_id": "doc_000002", "title": "FAISS Indexes", "text": "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.\n\nThe most basic FAISS index is the `IndexFlatL2`. It performs an exhaustive search, computing the L2 distance between the query vector and every other vector in the index. This is a brute-force approach and serves as the gold standard for accuracy. However, its search time scales linearly with the number of documents, making it slow for large datasets.\n\nFor larger datasets, FAISS offers methods that trade some accuracy for speed. A popular choice is the `IndexIVFPQ`. This index works by partitioning the vector space into cells (voronoi cells).\n- IVF (Inverted File): At search time, the index only searches within a few cells closest to the query vector, rather than the entire dataset. The `nprobe` parameter controls how many cells to visit. A higher `nprobe` means better accuracy but slower search.\n- PQ (Product Quantization): This technique compresses the vectors to reduce their memory footprint. It breaks down a large vector into smaller sub-vectors and quantizes each sub-vector separately. This allows FAISS to store massive datasets in RAM.\n\nIn summary, `IndexFlatL2` guarantees finding the exact nearest neighbors but is slow. `IndexIVFPQ` is much faster and more memory-efficient because it uses an approximate search strategy with partitioning and compression, but the results are not guaranteed to be perfect. The trade-off between speed and accuracy is controlled by parameters like `nprobe` and the PQ settings."}
{"doc_id": "doc_000003", "title": "Retrieval-Augmented Generation (RAG)", "text": "Retrieval-Augmented Generation (RAG) is a methodology for improving the performance of Large Language Models (LLMs) on knowledge-intensive tasks. It combines a pre-trained parametric memory (the LLM itself) with a non-parametric memory, which is typically a dense vector index of a large corpus like Wikipedia.\n\nThe core idea is to retrieve relevant documents from the external knowledge source and provide them as context to the LLM when generating an answer. This has several advantages. First, it allows the model to access up-to-date, real-time information that was not available in its training data. Second, it provides a way to ground the model's generations in verifiable evidence, reducing the risk of hallucination and making the outputs more trustworthy.\n\nThe RAG framework consists of two main components: a retriever and a generator.\n- The Retriever: Given a user input (e.g., a question), the retriever's job is to find a small set of relevant text passages from the knowledge base. This is often implemented using a dense passage retriever (DPR) model, which embeds both the input and the passages into a shared high-dimensional vector space for similarity search.\n- The Generator: This is a sequence-to-sequence model (like BART or T5) that takes the original user input and the retrieved passages as context, and generates the final text output. The generator learns to synthesize an answer by conditioning on the provided knowledge.\n\nRAG models can be trained end-to-end, allowing both the retriever and the generator to be fine-tuned together for the specific task. This joint training helps the retriever learn to find passages that are most useful for the generator."}
{"doc_id": "doc_000004", "title": "Dense Retrieval", "text": "Dense retrieval is a method used in information retrieval that represents documents and queries as dense vectors, known as embeddings. Unlike sparse retrieval methods like TF-IDF or BM25, which rely on keyword matching, dense retrieval captures the semantic meaning of the text.\n\nThe process begins with an embedding model, often a pre-trained transformer like BERT or a specialized model like Voyage, which maps text to a high-dimensional vector space. In this space, texts with similar meanings are located closer to each other. When a query comes in, it is also embedded into the same vector space. The retrieval system then searches for the document vectors that are closest to the query vector, typically using a similarity measure like cosine similarity or dot product.\n\nThe key advantage of dense retrieval is its ability to handle synonyms and semantically related terms. For example, a search for \"how to make a car go faster\" could retrieve a document about \"automobile performance tuning,\" even if they don't share keywords. This is because the embedding model understands the underlying concepts.\n\nHowever, dense retrieval can sometimes struggle with queries that require exact keyword matching, such as searching for a specific error code or a person's name. This is where hybrid approaches, which combine dense and sparse retrieval, can be particularly effective."}
{"doc_id": "doc_000005", "title": "RAG Evaluation", "text": "Evaluating a Retrieval-Augmented Generation (RAG) application involves assessing the quality of both the retrieval and generation components. A useful framework for this is the RAG Triad, which focuses on three key metrics: Answer Relevance, Context Relevance, and Groundedness.\n\n1.  **Context Relevance**: This metric measures how relevant the retrieved context is to the user's query. It answers the question: \"Does the retrieved information have a high signal-to-noise ratio for answering the query?\" If the context is irrelevant, the generator will struggle to produce a good answer, even if the generator itself is powerful. This is often the first and most important bottleneck to address.\n\n2.  **Groundedness**: This metric assesses how well the generated answer is supported by the retrieved context. It helps detect \"hallucinations,\" where the model makes up information that is not present in the provided text. A high groundedness score means the answer is based on the evidence provided, making it more factual and trustworthy.\n\n3.  **Answer Relevance**: This measures how relevant the final generated answer is to the user's original query. It ensures that the model is not just producing a factually correct statement but is actually addressing the specific question that was asked.\n\nBy evaluating these three components separately, developers can pinpoint weaknesses in their RAG system. For example, low context relevance points to a problem with the retriever, while low groundedness with high context relevance points to a problem with the generator's synthesis process."}
{"doc_id": "doc_000006", "title": "Hybrid Search", "text": "Hybrid search is an approach that combines the strengths of traditional keyword-based (sparse) search with modern semantic (dense) vector search. This combination provides more relevant and accurate results than either method could achieve on its own.\n\nSparse retrieval, powered by algorithms like BM25, excels at finding documents with exact keyword matches. It's highly effective for queries containing specific terms, acronyms, or identifiers. However, it can fail when queries use different wording or synonyms for the concepts present in the documents.\n\nDense retrieval, on the other hand, uses machine learning models to create vector embeddings that capture the semantic meaning of text. This allows it to find conceptually related documents even if they don't share the same keywords. Its weakness, however, is that it can sometimes miss documents with important keyword matches if the semantic meaning is not perfectly aligned.\n\nHybrid search resolves this by running both types of queries simultaneously. The results from each are then combined using a fusion algorithm, such as Reciprocal Rank Fusion (RRF). RRF is a technique that ranks the combined results in a way that balances both the keyword and semantic relevance. The result is a search system that is both precise with keywords and intelligent about meaning, delivering a superior user experience."}
{"doc_id": "doc_000007", "title": "agentic decomposition", "text": "# Agentic Query Decomposition\n\nBreaking complex questions into sub-queries can improve coverage. However, avoid over-decomposition that dilutes signal. Limit to 5–10 sub-queries, deduplicate contexts, and synthesize with explicit instructions to ground the final answer.\n"}
{"doc_id": "doc_000008", "title": "agentic limits", "text": "# Agentic Limits\n\nDecompose only when necessary to avoid drifting into irrelevant sub-queries; cap at ~10.\n"}
{"doc_id": "doc_000009", "title": "azure vs aws", "text": "# Azure vs AWS for RAG\n\nBoth clouds offer managed vector stores, serverless, and GPUs. Choose based on existing agreements and data residency. On Azure, combine Databricks with Azure OpenAI/Voyage and Blob/Delta. On AWS, use Databricks with Bedrock/Voyage and S3/Delta. Terraform abstracts differences via providers.\n"}
{"doc_id": "doc_000010", "title": "bm25 basics", "text": "# BM25 Basics\n\nBM25 scores documents based on term frequency, inverse document frequency, and document length normalization. It excels at exact keyword matching, making it complementary to dense retrieval. Tokenization quality and stopword handling significantly influence performance.\n"}
{"doc_id": "doc_000011", "title": "bm25 tokenization", "text": "# BM25 Tokenization\n\nCustomize tokenization for hyphens, underscores, and code identifiers; extend stopword lists for your domain.\n"}
{"doc_id": "doc_000012", "title": "bm25 tuning", "text": "# BM25 Tuning\n\nTokenization, stopwords, and stemming strongly affect BM25. Consider domain-specific tokenization and custom stopword lists. For code/doc mixtures, splitting on punctuation and camelCase can help. Validate with held-out queries.\n"}
{"doc_id": "doc_000013", "title": "bm25 vs dense", "text": "# BM25 vs Dense\n\nBM25 excels at exact tokens and codes; dense handles paraphrase. Hybrid mitigates both weaknesses.\n"}
{"doc_id": "doc_000014", "title": "chunking strategies", "text": "# Chunking Strategies\n\nChunk granularity influences recall and precision. Smaller chunks improve precision and reduce noise but may fragment context. Overlap preserves coherence. Adding lightweight metadata (titles, headings) as context can guide rerankers. Empirical tuning of sentence windows and overlap often yields gains.\n"}
{"doc_id": "doc_000015", "title": "colbert gpu", "text": "# ColBERT on GPU\n\nSignificant latency reductions; ensure CUDA versions match PyTorch and Transformers.\n"}
{"doc_id": "doc_000016", "title": "colbert latency", "text": "# ColBERT Latency Considerations\n\nColBERT incurs per-query encoding and MaxSim computation. To reduce latency: cache frequent queries, cap reranker_k, and ensure efficient batching. On GPU, throughput improves substantially. For small candidate sets, CPU can suffice.\n"}
{"doc_id": "doc_000017", "title": "colbert overview", "text": "# ColBERT Overview\n\nColBERT (Contextualized Late Interaction over BERT) is a retrieval model that balances the expressiveness of cross-encoders with the efficiency of bi-encoders by introducing late interaction. Queries and documents are encoded into token-level embeddings. At search time, ColBERT computes MaxSim between each query token embedding and document token embeddings, then sums these maxima to produce a score. This preserves token-level matching while allowing efficient indexing.\nKey properties:\n- Late Interaction: preserves rich token semantics during retrieval.\n- Efficiency: document embeddings can be precomputed and compressed.\n- Accuracy: more precise than single-vector bi-encoders on many IR tasks.\nCommon use: reranking small candidate sets or building ColBERT-specific ANN indexes.\n"}
{"doc_id": "doc_000018", "title": "colbert practical tips", "text": "# ColBERT Practical Tips\n\nBatch queries, cap doc_maxlen, and cache frequent passages. Monitor MaxSim distributions and layer norms for anomalies.\n"}
{"doc_id": "doc_000019", "title": "colbert vs biencoder", "text": "# ColBERT vs Bi-Encoders\n\nBi-encoders represent queries and documents as single dense vectors, enabling fast ANN search but losing token-level nuance. ColBERT uses multiple vectors per text and late interaction, improving sensitivity to exact terms and phrasings. This typically increases recall and MRR over bi-encoders on passage retrieval benchmarks while keeping latency manageable.\n"}
{"doc_id": "doc_000020", "title": "colbert vs crossencoder", "text": "# ColBERT vs Cross-Encoders\n\nCross-encoders jointly encode the [query, document] pair with full attention, enabling deep interactions but at high computational cost. They excel in reranking but are infeasible for exhaustive retrieval at scale. ColBERT decouples encoding and uses a MaxSim late interaction at query time, yielding faster retrieval while retaining fine-grained matching. Cross-encoders often achieve the highest accuracy per pair, but ColBERT provides a practical accuracy/speed trade-off for retrieval and reranking.\n"}
{"doc_id": "doc_000021", "title": "context window limits", "text": "# Context Window Limits\n\nStay within generator context limits; summarize or trim low-signal passages when needed.\n"}
{"doc_id": "doc_000022", "title": "context windowing", "text": "# Context Windowing\n\nRerankers benefit from clean passages. Keep chunks within model context limits; for ColBERT, doc_maxlen ~180 is common. For generator LLMs, prune low-signal passages and summarize when necessary to fit within context.\n"}
{"doc_id": "doc_000023", "title": "cost optimization", "text": "# Cost Optimization\n\nCost drivers include embedding volume, reranker inference, and generator tokens. Batch embeddings, cache repeats, limit reranker_k, and use small yet capable LLMs for evaluation. For infra, autoscale, spot/preemptible instances, and consider serverless endpoints for bursty workloads.\n"}
{"doc_id": "doc_000024", "title": "data quality", "text": "# Data Quality\n\nPrefer authoritative, concise docs; remove duplicates; enforce consistent titles and doc_ids.\n"}
{"doc_id": "doc_000025", "title": "databricks overview", "text": "# Databricks for RAG\n\nDatabricks provides managed compute, Delta Lake storage, MLflow tracking, and Unity Catalog governance. For RAG: orchestrate ETL to curate corpora, run embedding jobs at scale, store vectors/metadata, and serve retrieval with model-serving. Terraform can provision clusters, storage, and permissions consistently across clouds.\n"}
{"doc_id": "doc_000026", "title": "dense vector norms", "text": "# Vector Normalization\n\nNormalize L2 for inner-product search; ensure consistent preprocessing at index and query time.\n"}
{"doc_id": "doc_000027", "title": "embedding models voyage", "text": "# Embedding Models (Voyage)\n\nVoyage models map text to dense vectors. Key considerations:\n- Dimensionality: affects index size and compute.\n- Throughput/latency: batch requests to improve efficiency.\n- Domain adaptation: specialty models may score higher on niche corpora.\nFor RAG, ensure consistent model usage for index build and querying.\n"}
{"doc_id": "doc_000028", "title": "eval canary queries", "text": "# Canary Queries\n\nInclude queries targeting rare tokens, synonyms, and numeric identifiers to detect regressions early.\n"}
{"doc_id": "doc_000029", "title": "eval methodology", "text": "# Evaluation Methodology\n\nUse realistic queries and track metrics over time. For RAG triad, keep prompts stable. Compare retrieval-only metrics (recall@K, MRR) separately from end-to-end metrics. Add canary queries to detect regressions in sparse or dense components.\n"}
{"doc_id": "doc_000030", "title": "faiss basics", "text": "# FAISS Basics\n\nFAISS provides vector indexes for similarity search. IndexFlatL2 and IndexFlatIP perform exact search and serve as accuracy baselines. For large datasets, approximate indexes like IVF, HNSW, and PQ reduce latency and memory. FAISS supports GPU acceleration, training (e.g., k-means for IVF), and quantization techniques for compact storage.\n"}
{"doc_id": "doc_000031", "title": "faiss hnsw", "text": "# FAISS HNSW\n\nHNSW builds a multi-layer navigable small-world graph. During search, the algorithm traverses from an entry point, refining candidates through neighbors at each layer. Parameters like efSearch and efConstruction control accuracy vs. latency and build time. HNSW performs well for high-recall approximate search with strong empirical performance.\n"}
{"doc_id": "doc_000032", "title": "faiss ivf training", "text": "# IVF Training Data\n\nAim for ≥4×nlist training samples; otherwise prefer Flat or HNSW to avoid poor centroids.\n"}
{"doc_id": "doc_000033", "title": "faiss ivfpq", "text": "# FAISS IVFPQ\n\nIVFPQ combines an inverted file (IVF) with product quantization (PQ). IVF partitions the space into nlist clusters; queries probe nprobe nearest clusters. PQ compresses residual vectors into subvector codebooks. Trade-offs:\n- Pros: memory-efficient, fast search over large corpora.\n- Cons: approximate results, requires sufficient training data (often ≥ 4×nlist samples).\nKey params: nlist (coarse cells), m (PQ subvectors), nprobe (clusters visited at query time).\n"}
{"doc_id": "doc_000034", "title": "faiss metrics", "text": "# FAISS Metrics\n\nMeasure recall@K vs. Flat, QPS, and memory footprint. Validate on held-out queries before changing nlist/nprobe.\n"}
{"doc_id": "doc_000035", "title": "faiss tuning", "text": "# FAISS Tuning\n\nFor IVFPQ, ensure sufficient training samples relative to nlist. Increase nprobe for higher recall at the cost of latency. Consider Flat or HNSW for small corpora. Normalize vectors if using inner product. Measure recall@K against a brute-force baseline to validate.\n"}
{"doc_id": "doc_000036", "title": "hybrid benefits", "text": "# Benefits of Hybrid\n\nHybrid retrieval improves robustness: sparse catches exact strings (IDs, codes), dense captures paraphrases. RRF is a strong baseline; learned fusion can add modest gains but requires labels. Hybrid also helps when one modality underperforms due to domain shift.\n"}
{"doc_id": "doc_000037", "title": "hybrid rrf k", "text": "# RRF k Parameter\n\nTypical k=60 balances aggressiveness; tune per corpus size. Larger k dampens rank differences.\n"}
{"doc_id": "doc_000038", "title": "hybrid search", "text": "# Hybrid Search\n\nHybrid search fuses dense semantic results with sparse lexical results (e.g., BM25). Reciprocal Rank Fusion (RRF) is a simple, robust technique to combine rank lists. Benefits:\n- Sparse captures exact terms and rare tokens.\n- Dense captures paraphrases and semantics.\n- Fusion improves robustness across query types.\n"}
{"doc_id": "doc_000039", "title": "index size considerations", "text": "# Index Size Considerations\n\nVector dimensionality and number of chunks drive memory. PQ reduces footprint but may reduce accuracy. Track index size and latency alongside quality to find acceptable trade-offs.\n"}
{"doc_id": "doc_000040", "title": "index update strategies", "text": "# Index Updates\n\nUse periodic full rebuilds or append-only with periodic compaction; version artifacts for rollback.\n"}
{"doc_id": "doc_000041", "title": "infra costs", "text": "# Infra Costs\n\nBatch embedding jobs, use spot instances, and autoscale serving layers to control spend.\n"}
{"doc_id": "doc_000042", "title": "kubernetes role", "text": "# Kubernetes Role in RAG\n\nKubernetes is optional. Managed services (Databricks model serving, serverless functions) can suffice. Kubernetes helps if you need custom vector services, GPUs, or multi-tenant workloads. Helm charts and HPA enable repeatability and autoscaling.\n"}
{"doc_id": "doc_000043", "title": "logging observability", "text": "# Observability\n\nLog query latency breakdown (dense, sparse, fusion, reranker) and top-k overlaps.\n"}
{"doc_id": "doc_000044", "title": "metadata design", "text": "# Metadata Design\n\nInclude titles, headings, and source identifiers. Metadata aids rerankers and UX (citations). Preserve document boundaries and stable doc_ids to avoid evaluation drift.\n"}
{"doc_id": "doc_000045", "title": "negative sampling", "text": "# Negative Sampling\n\nFor training or tuning rerankers, hard negatives improve discrimination. Even in evaluation, include queries that test edge cases to avoid overfitting to easy wins.\n"}
{"doc_id": "doc_000046", "title": "prompt citations", "text": "# Prompting with Citations\n\nAsk for quotations and cite doc_ids. Enforce that each claim references retrieved evidence.\n"}
{"doc_id": "doc_000047", "title": "prompt grounding", "text": "# Prompting for Grounding\n\nGrounded answers cite or reflect evidence. Techniques:\n- Ask the model to quote, cite, or list supporting snippets.\n- Penalize unsupported claims in system prompts.\n- Use constrained generation with retrieved spans (extract-then-generate).\nThese often improve groundedness in the RAG triad.\n"}
{"doc_id": "doc_000048", "title": "rag triad", "text": "# RAG Triad Evaluation\n\nThe RAG Triad evaluates: (1) Context Relevance (retrieval quality), (2) Groundedness (answer supported by context), and (3) Answer Relevance (final answer addresses the query). Improvements often come from better retrievers (context relevance), citation/attribution prompting (groundedness), and instruction tuning (answer relevance).\n"}
{"doc_id": "doc_000049", "title": "reranker k tuning", "text": "# Tuning reranker_k\n\nValues 10–30 often suffice; increasing beyond may add noise and latency.\n"}
{"doc_id": "doc_000050", "title": "reranking overview", "text": "# Reranking Overview\n\nRerankers refine candidate lists from fast retrievers. Cross-encoders provide strong pairwise scoring but are compute-heavy. ColBERT offers a middle ground with token-level late interaction. Selecting reranker depends on latency budget and quality target.\n"}
{"doc_id": "doc_000051", "title": "retrieval failures", "text": "# Common Retrieval Failures\n\nFailure modes include:\n- Vocabulary mismatch: dense helps; add synonyms, augment data.\n- Ambiguity: query decomposition or disambiguation prompts.\n- Over-chunking: too small windows fragment context.\n- Poor metadata: missing titles harm reranking.\nDiagnostics: manual query audits, coverage tests, and recall at K.\n"}
{"doc_id": "doc_000052", "title": "rrf fusion", "text": "# Reciprocal Rank Fusion\n\nRRF merges multiple ranked lists by summing 1/(k + rank). It is parameter-light and robust to noise. Typical k ranges 10–60. RRF often improves recall in hybrid search by elevating candidates appearing in either dense or sparse lists.\n"}
{"doc_id": "doc_000053", "title": "security controls", "text": "# Security Controls\n\nManage secrets via vaults, restrict egress, and audit API usage; redact PII early.\n"}
{"doc_id": "doc_000054", "title": "security privacy", "text": "# Security & Privacy\n\nWhen sending queries to external APIs (embeddings, LLMs), consider PII handling, data minimization, and encryption in transit. Offer opt-out and redaction for sensitive fields.\n"}
{"doc_id": "doc_000055", "title": "terraform basics", "text": "# Terraform Basics\n\nTerraform codifies infrastructure. Core concepts: providers, resources, variables, and state. Modules enable reuse. For a RAG stack, define modules for VPC/networking, Databricks workspace, clusters, storage, and secrets. CI plans and applies changes with approvals.\n"}
